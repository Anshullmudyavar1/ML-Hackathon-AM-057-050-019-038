{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3c2a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMPROVED HMM WITH BIGRAM MODELING\n",
    "==================================\n",
    "Incorporates the bigram transition approach from the high-performing solution.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "790b7baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "class ImprovedHangmanHMM:\n",
    "    \"\"\"\n",
    "    Enhanced HMM that uses an interpolated trigram/bigram/unigram model,\n",
    "    dynamically blended with a pattern-matching model for predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, smoothing=1.0):\n",
    "        self.smoothing = smoothing\n",
    "        self.alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        self.letter_to_idx = {c: i for i, c in enumerate(self.alphabet)}\n",
    "        self.idx_to_letter = {i: c for i, c in enumerate(self.alphabet)}\n",
    "        \n",
    "        # Probabilities for Unigram, Bigram, and Trigram models\n",
    "        self.unigram_probs = np.ones(26) / 26\n",
    "        self.bigram_probs = np.ones((26, 26)) / 26      # P(w_i | w_{i-1})\n",
    "        self.trigram_probs = np.ones((26, 26, 26)) / 26 # P(w_i | w_{i-2}, w_{i-1})\n",
    "        \n",
    "        # Word storage for the pattern-matching sub-model\n",
    "        self.words_by_length = {}\n",
    "        self.trained = False\n",
    "    \n",
    "    def train(self, words):\n",
    "        \"\"\"Train the unigram, bigram, and trigram models on the corpus.\"\"\"\n",
    "        print(\"\\nTraining HMM with Trigram, Bigram, and Unigram models...\")\n",
    "        \n",
    "        # Initialize counts with smoothing to avoid zero probabilities\n",
    "        unigram_counts = np.full(26, self.smoothing)\n",
    "        bigram_counts = np.full((26, 26), self.smoothing)\n",
    "        trigram_counts = np.full((26, 26, 26), self.smoothing)\n",
    "        \n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if not word or not all(c in self.alphabet for c in word):\n",
    "                continue\n",
    "            \n",
    "            # Store word for pattern matching\n",
    "            length = len(word)\n",
    "            if length not in self.words_by_length:\n",
    "                self.words_by_length[length] = []\n",
    "            self.words_by_length[length].append(word)\n",
    "\n",
    "            # Count unigrams\n",
    "            for char in word:\n",
    "                unigram_counts[self.letter_to_idx[char]] += 1.0\n",
    "            \n",
    "            # Count bigrams (w_{i-1} -> w_i)\n",
    "            for i in range(len(word) - 1):\n",
    "                prev_idx = self.letter_to_idx[word[i]]\n",
    "                curr_idx = self.letter_to_idx[word[i+1]]\n",
    "                bigram_counts[prev_idx, curr_idx] += 1.0\n",
    "            \n",
    "            # Count trigrams (w_{i-2}, w_{i-1} -> w_i)\n",
    "            for i in range(len(word) - 2):\n",
    "                p_prev_idx = self.letter_to_idx[word[i]]\n",
    "                prev_idx = self.letter_to_idx[word[i+1]]\n",
    "                curr_idx = self.letter_to_idx[word[i+2]]\n",
    "                trigram_counts[p_prev_idx, prev_idx, curr_idx] += 1.0\n",
    "        \n",
    "        # Normalize all counts to probabilities, adding a small epsilon to avoid division by zero\n",
    "        self.unigram_probs = unigram_counts / unigram_counts.sum()\n",
    "        self.bigram_probs = bigram_counts / (bigram_counts.sum(axis=1, keepdims=True) + 1e-9)\n",
    "        self.trigram_probs = trigram_counts / (trigram_counts.sum(axis=2, keepdims=True) + 1e-9)\n",
    "        \n",
    "        self.trained = True\n",
    "        print(f\"Trained on {len(words)} words.\")\n",
    "        print(f\"Corpus contains words of lengths: {sorted(self.words_by_length.keys())}\")\n",
    "\n",
    "    def _get_pattern_probs(self, masked_word, guessed_letters):\n",
    "        \"\"\"Helper function to get probabilities from filtering the corpus.\"\"\"\n",
    "        length = len(masked_word)\n",
    "        pattern_probs = np.zeros(26)\n",
    "        num_matching_words = 0\n",
    "\n",
    "        if length in self.words_by_length:\n",
    "            pattern = masked_word.replace('_', '.')\n",
    "            try:\n",
    "                pattern_regex = re.compile(f'^{pattern}$')\n",
    "            except re.error:\n",
    "                return pattern_probs, 0 # Invalid regex pattern\n",
    "\n",
    "            guessed_wrong = {l for l in guessed_letters if l not in masked_word}\n",
    "            \n",
    "            matching_words = [\n",
    "                w for w in self.words_by_length[length] \n",
    "                if pattern_regex.match(w) and not any(c in guessed_wrong for c in w)\n",
    "            ]\n",
    "            num_matching_words = len(matching_words)\n",
    "\n",
    "            if matching_words:\n",
    "                # Count unguessed letters in the filtered word list\n",
    "                letter_counts = Counter(c for w in matching_words for c in set(w) if c not in guessed_letters)\n",
    "                if letter_counts:\n",
    "                    total_counts = sum(letter_counts.values())\n",
    "                    for letter, count in letter_counts.items():\n",
    "                        pattern_probs[self.letter_to_idx[letter]] = count / total_counts\n",
    "        \n",
    "        return pattern_probs, num_matching_words\n",
    "\n",
    "    def predict_letter_probabilities(self, masked_word, guessed_letters):\n",
    "        \"\"\"\n",
    "        Predicts letter probabilities using a dynamic blend of pattern-matching \n",
    "        and an interpolated n-gram model.\n",
    "        \"\"\"\n",
    "        if not self.trained:\n",
    "            return np.ones(26) / 26\n",
    "        \n",
    "        # --- 1. Get probabilities from the INTERPOLATED N-GRAM MODEL ---\n",
    "        # This model scores each potential letter based on its preceding context.\n",
    "        lambda1, lambda2, lambda3 = 0.1, 0.3, 0.6  # Weights for unigram, bigram, trigram\n",
    "        presence_probs = np.zeros(26)\n",
    "        \n",
    "        num_blanks = masked_word.count('_')\n",
    "        if num_blanks == 0:\n",
    "            return np.zeros(26)\n",
    "\n",
    "        for i, char in enumerate(masked_word):\n",
    "            if char == '_':\n",
    "                # Get context: the two letters immediately preceding the blank\n",
    "                prev_char = masked_word[i-1] if i > 0 and masked_word[i-1] != '_' else None\n",
    "                p_prev_char = masked_word[i-2] if i > 1 and masked_word[i-2] != '_' else None\n",
    "                \n",
    "                prob_dist = self.unigram_probs # Default to unigram\n",
    "                \n",
    "                # If one preceding letter is known, interpolate bigram and unigram\n",
    "                if prev_char:\n",
    "                    prev_idx = self.letter_to_idx[prev_char]\n",
    "                    bigram_dist = self.bigram_probs[prev_idx, :]\n",
    "                    prob_dist = (1 - lambda1) * bigram_dist + lambda1 * self.unigram_probs\n",
    "                \n",
    "                # If two preceding letters are known, use the full trigram interpolation\n",
    "                if p_prev_char and prev_char:\n",
    "                    p_prev_idx = self.letter_to_idx[p_prev_char]\n",
    "                    prev_idx = self.letter_to_idx[prev_char]\n",
    "                    trigram_dist = self.trigram_probs[p_prev_idx, prev_idx, :]\n",
    "                    bigram_dist = self.bigram_probs[prev_idx, :]\n",
    "                    prob_dist = (lambda3 * trigram_dist) + (lambda2 * bigram_dist) + (lambda1 * self.unigram_probs)\n",
    "                \n",
    "                presence_probs += prob_dist\n",
    "\n",
    "        # Normalize the summed probabilities from all blank positions\n",
    "        if presence_probs.sum() > 0:\n",
    "            presence_probs /= presence_probs.sum()\n",
    "\n",
    "        # --- 2. Get probabilities from PATTERN MATCHING ---\n",
    "        pattern_probs, num_matching_words = self._get_pattern_probs(masked_word, guessed_letters)\n",
    "\n",
    "        # --- 3. DYNAMICALLY BLEND the two models ---\n",
    "        if pattern_probs.sum() > 0:\n",
    "            # If pattern matching is highly specific (few matching words), trust it more.\n",
    "            pattern_weight = max(0.5, min(0.95, 0.95 - (num_matching_words / 500.0)))\n",
    "            combined = pattern_weight * pattern_probs + (1.0 - pattern_weight) * presence_probs\n",
    "        else:\n",
    "            # If no words match the pattern, rely solely on the n-gram model\n",
    "            combined = presence_probs\n",
    "        \n",
    "        # --- 4. Final Cleanup ---\n",
    "        # Ensure already guessed letters have a probability of 0\n",
    "        for letter in guessed_letters:\n",
    "            if letter in self.letter_to_idx:\n",
    "                combined[self.letter_to_idx[letter]] = 0\n",
    "        \n",
    "        # Final normalization to ensure probabilities sum to 1\n",
    "        if combined.sum() > 0:\n",
    "            combined /= combined.sum()\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def save(self, filename='improved_hmm_model.pkl'):\n",
    "        \"\"\"Saves the trained HMM model to a file.\"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'unigram_probs': self.unigram_probs,\n",
    "                'bigram_probs': self.bigram_probs,\n",
    "                'trigram_probs': self.trigram_probs,\n",
    "                'words_by_length': self.words_by_length,\n",
    "                'alphabet': self.alphabet,\n",
    "                'letter_to_idx': self.letter_to_idx,\n",
    "                'smoothing': self.smoothing\n",
    "            }, f)\n",
    "        print(f\"Model saved to {filename}\")\n",
    "    \n",
    "    def load(self, filename='improved_hmm_model.pkl'):\n",
    "        \"\"\"Loads a pre-trained HMM model from a file.\"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.unigram_probs = data['unigram_probs']\n",
    "            self.bigram_probs = data['bigram_probs']\n",
    "            self.trigram_probs = data['trigram_probs']\n",
    "            self.words_by_length = data['words_by_length']\n",
    "            self.alphabet = data['alphabet']\n",
    "            self.letter_to_idx = data['letter_to_idx']\n",
    "            self.smoothing = data['smoothing']\n",
    "            self.trained = True\n",
    "        print(f\"Model loaded from {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23074285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HANGMAN HMM TRAINER (Trigram Version)\n",
      "======================================================================\n",
      "Loaded 50000 words from corpus.txt\n",
      "\n",
      "Training HMM with Trigram, Bigram, and Unigram models...\n",
      "Trained on 50000 words.\n",
      "Corpus contains words of lengths: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "Model saved to improved_hmm_model.pkl\n",
      "\n",
      "======================================================================\n",
      "TESTING HMM PREDICTIONS\n",
      "======================================================================\n",
      "\n",
      "Masked: puzzl_, Guessed: {'l', 'p', 'u', 'z'}\n",
      "Top 5 letter predictions:\n",
      "  e: 0.3350\n",
      "  i: 0.0965\n",
      "  a: 0.0844\n",
      "  y: 0.0820\n",
      "  o: 0.0728\n",
      "\n",
      "Masked: _ing, Guessed: {'g', 'i', 'n'}\n",
      "Top 5 letter predictions:\n",
      "  r: 0.2416\n",
      "  t: 0.2414\n",
      "  h: 0.2390\n",
      "  j: 0.2373\n",
      "  e: 0.0065\n",
      "\n",
      "Masked: q__, Guessed: {'q'}\n",
      "Top 5 letter predictions:\n",
      "  u: 0.2127\n",
      "  i: 0.1911\n",
      "  a: 0.0971\n",
      "  o: 0.0966\n",
      "  c: 0.0958\n",
      "\n",
      "Masked: appl_, Guessed: {'a', 'l', 'p'}\n",
      "Top 5 letter predictions:\n",
      "  e: 0.3482\n",
      "  i: 0.2134\n",
      "  o: 0.1524\n",
      "  u: 0.0815\n",
      "  y: 0.0731\n",
      "\n",
      "Masked: pytho_, Guessed: {'y', 't', 'h', 'o', 'p'}\n",
      "Top 5 letter predictions:\n",
      "  m: 0.4814\n",
      "  q: 0.4779\n",
      "  r: 0.0081\n",
      "  n: 0.0072\n",
      "  l: 0.0056\n",
      "\n",
      "======================================================================\n",
      "HMM TRAINING AND TESTING COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"HANGMAN HMM TRAINER (Trigram Version)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load the corpus file\n",
    "    try:\n",
    "        with open('corpus.txt', 'r') as f:\n",
    "            words = [line.strip().lower() for line in f if line.strip()]\n",
    "        print(f\"Loaded {len(words)} words from corpus.txt\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: corpus.txt not found. Please ensure it's in the same directory.\")\n",
    "        exit()\n",
    "    \n",
    "    # Initialize and train the HMM\n",
    "    hmm = ImprovedHangmanHMM(smoothing=1.0)\n",
    "    hmm.train(words)\n",
    "    \n",
    "    # Save the trained model\n",
    "    hmm.save('improved_hmm_model.pkl')\n",
    "    \n",
    "    # --- Test the model with some example cases ---\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TESTING HMM PREDICTIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    test_cases = [\n",
    "        (\"puzzl_\", {'p','u','z','z','l'}),\n",
    "        (\"_ing\", {'i','n','g'}),\n",
    "        (\"q__\", {'q'}),\n",
    "        (\"appl_\", {'a','p','p','l'}),\n",
    "        (\"pytho_\", {'p','y','t','h','o'}),\n",
    "    ]\n",
    "    \n",
    "    for masked, guessed in test_cases:\n",
    "        probs = hmm.predict_letter_probabilities(masked, guessed)\n",
    "        top_5 = sorted(enumerate(probs), key=lambda x: x[1], reverse=True)[:5]\n",
    "        \n",
    "        print(f\"\\nMasked: {masked}, Guessed: {guessed}\")\n",
    "        print(\"Top 5 letter predictions:\")\n",
    "        for idx, prob in top_5:\n",
    "            if prob > 0:\n",
    "                print(f\"  {hmm.idx_to_letter[idx]}: {prob:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"HMM TRAINING AND TESTING COMPLETE!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feb97c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 3: TESTING AND VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "def test_hmm(hmm, test_cases):\n",
    "    \"\"\"Test the HMM on some example cases.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TESTING HMM PREDICTIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for masked_word, guessed in test_cases:\n",
    "        probs = hmm.predict_letter_probabilities(masked_word, set(guessed))\n",
    "        \n",
    "        # Get top 5 predictions\n",
    "        top_indices = np.argsort(probs)[-5:][::-1]\n",
    "        top_letters = [(hmm.alphabet[i], probs[i]) for i in top_indices]\n",
    "        \n",
    "        print(f\"\\nMasked word: {masked_word}\")\n",
    "        print(f\"Guessed: {guessed if guessed else 'none'}\")\n",
    "        print(f\"Top 5 predictions:\")\n",
    "        for letter, prob in top_letters:\n",
    "            print(f\"  {letter}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db62961c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HANGMAN HMM TRAINER\n",
      "======================================================================\n",
      "Loaded 50000 words from corpus\n",
      "Word length range: 1 to 24\n",
      "Most common lengths: [(9, 6808), (10, 6465), (8, 6348), (11, 5452), (7, 5111), (12, 4292), (6, 3755), (13, 3094), (5, 2340), (14, 2019)]\n",
      "\n",
      "Letter frequencies (top 10):\n",
      "  e: 49224 (10.37%)\n",
      "  a: 42110 (8.87%)\n",
      "  i: 42068 (8.86%)\n",
      "  o: 35829 (7.54%)\n",
      "  r: 33619 (7.08%)\n",
      "  n: 33314 (7.02%)\n",
      "  t: 32191 (6.78%)\n",
      "  s: 29044 (6.12%)\n",
      "  l: 27406 (5.77%)\n",
      "  c: 21718 (4.57%)\n",
      "\n",
      "Training HMMs...\n",
      "  Length 11: 5452 words\n",
      "  Length 6: 3755 words\n",
      "  Length 9: 6808 words\n",
      "  Length 14: 2019 words\n",
      "  Length 10: 6465 words\n",
      "  Length 8: 6348 words\n",
      "  Length 12: 4292 words\n",
      "  Length 13: 3094 words\n",
      "  Length 5: 2340 words\n",
      "  Length 4: 1169 words\n",
      "  Length 3: 388 words\n",
      "  Length 7: 5111 words\n",
      "  Length 15: 1226 words\n",
      "  Length 2: 84 words\n",
      "  Length 1: 46 words\n",
      "  Length 20: 40 words\n",
      "Trained models for 21 different word lengths\n",
      "Model saved to hmm_model.pkl\n",
      "\n",
      "======================================================================\n",
      "TESTING HMM PREDICTIONS\n",
      "======================================================================\n",
      "\n",
      "Masked word: _____\n",
      "Guessed: none\n",
      "Top 5 predictions:\n",
      "  a: 0.1069\n",
      "  e: 0.0981\n",
      "  o: 0.0695\n",
      "  r: 0.0674\n",
      "  i: 0.0635\n",
      "\n",
      "Masked word: _pp__\n",
      "Guessed: p\n",
      "Top 5 predictions:\n",
      "  e: 0.5021\n",
      "  i: 0.2502\n",
      "  a: 0.0257\n",
      "  s: 0.0233\n",
      "  t: 0.0194\n",
      "\n",
      "Masked word: ____\n",
      "Guessed: eaios\n",
      "Top 5 predictions:\n",
      "  t: 0.0986\n",
      "  r: 0.0898\n",
      "  n: 0.0891\n",
      "  l: 0.0884\n",
      "  u: 0.0691\n",
      "\n",
      "Masked word: a_____\n",
      "Guessed: a\n",
      "Top 5 predictions:\n",
      "  e: 0.1114\n",
      "  i: 0.0942\n",
      "  n: 0.0837\n",
      "  l: 0.0820\n",
      "  t: 0.0734\n",
      "\n",
      "Masked word: _a____\n",
      "Guessed: a\n",
      "Top 5 predictions:\n",
      "  e: 0.1026\n",
      "  r: 0.0897\n",
      "  l: 0.0782\n",
      "  n: 0.0739\n",
      "  t: 0.0698\n",
      "\n",
      "Masked word: __e_____\n",
      "Guessed: e\n",
      "Top 5 predictions:\n",
      "  r: 0.1045\n",
      "  a: 0.0846\n",
      "  o: 0.0790\n",
      "  i: 0.0772\n",
      "  t: 0.0707\n",
      "\n",
      "======================================================================\n",
      "HMM TRAINING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Model saved as 'hmm_model.pkl'\n",
      "You can now use this model in the RL agent.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"HANGMAN HMM TRAINER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load corpus\n",
    "    words = load_corpus('corpus.txt')\n",
    "    \n",
    "    # Analyze corpus\n",
    "    letter_freq = analyze_corpus(words)\n",
    "    \n",
    "    # Initialize and train HMM\n",
    "    hmm = HangmanHMM()\n",
    "    hmm.train(words)\n",
    "    \n",
    "    # Save the model\n",
    "    hmm.save('hmm_model.pkl')\n",
    "    \n",
    "    # Test cases\n",
    "    test_cases = [\n",
    "        (\"_____\", \"\"),           # Empty word, length 5\n",
    "        (\"_pp__\", \"p\"),          # Word with known letter\n",
    "        (\"____\", \"eaios\"),       # Many guessed letters\n",
    "        (\"a_____\", \"a\"),         # First letter known\n",
    "        (\"_a____\", \"a\"),         # Second letter known\n",
    "        (\"__e_____\", \"e\"),       # Longer word\n",
    "    ]\n",
    "    \n",
    "    test_hmm(hmm, test_cases)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"HMM TRAINING COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nModel saved as 'hmm_model.pkl'\")\n",
    "    print(\"You can now use this model in the RL agent.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AIML GPU)",
   "language": "python",
   "name": "aiml-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
