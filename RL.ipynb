{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9a3ba0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMPROVED RL AGENT\n",
    "=================\n",
    "Incorporates best practices from the high-performing solution:\n",
    "1. Balanced reward function\n",
    "2. Simpler feature representation\n",
    "3. More stable training\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "IMPROVED HMM WITH BIGRAM MODELING AND DYNAMIC WEIGHTING\n",
    "======================================================\n",
    "Incorporates the bigram transition approach and adds dynamic weighting between\n",
    "the sequence model and the pattern-matching model for more robust predictions.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# ==============================================================================\n",
    "# FINAL HMM CLASS DEFINITION (COPY THIS ENTIRE CLASS INTO YOUR RL NOTEBOOK)\n",
    "# ==============================================================================\n",
    "\n",
    "class ImprovedHangmanHMM:\n",
    "    \"\"\"\n",
    "    Enhanced HMM that uses an interpolated trigram/bigram/unigram model,\n",
    "    dynamically blended with a pattern-matching model for predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, smoothing=1.0):\n",
    "        self.smoothing = smoothing\n",
    "        self.alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        self.letter_to_idx = {c: i for i, c in enumerate(self.alphabet)}\n",
    "        self.idx_to_letter = {i: c for i, c in enumerate(self.alphabet)}\n",
    "        \n",
    "        ### CORRECTED ATTRIBUTES ###\n",
    "        self.unigram_probs = np.ones(26) / 26\n",
    "        self.bigram_probs = np.ones((26, 26)) / 26\n",
    "        self.trigram_probs = np.ones((26, 26, 26)) / 26\n",
    "        \n",
    "        self.words_by_length = {}\n",
    "        self.trained = False\n",
    "    \n",
    "    # The train method is not strictly needed in the RL notebook but is included for completeness.\n",
    "    def train(self, words):\n",
    "        print(\"\\nTraining HMM with Trigram, Bigram, and Unigram models...\")\n",
    "        unigram_counts = np.full(26, self.smoothing)\n",
    "        bigram_counts = np.full((26, 26), self.smoothing)\n",
    "        trigram_counts = np.full((26, 26, 26), self.smoothing)\n",
    "        \n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if not word or not all(c in self.alphabet for c in word): continue\n",
    "            length = len(word)\n",
    "            if length not in self.words_by_length: self.words_by_length[length] = []\n",
    "            self.words_by_length[length].append(word)\n",
    "            for char in word: unigram_counts[self.letter_to_idx[char]] += 1.0\n",
    "            for i in range(len(word) - 1):\n",
    "                prev_idx = self.letter_to_idx[word[i]]\n",
    "                curr_idx = self.letter_to_idx[word[i+1]]\n",
    "                bigram_counts[prev_idx, curr_idx] += 1.0\n",
    "            for i in range(len(word) - 2):\n",
    "                p_prev_idx = self.letter_to_idx[word[i]]\n",
    "                prev_idx = self.letter_to_idx[word[i+1]]\n",
    "                curr_idx = self.letter_to_idx[word[i+2]]\n",
    "                trigram_counts[p_prev_idx, prev_idx, curr_idx] += 1.0\n",
    "        \n",
    "        self.unigram_probs = unigram_counts / unigram_counts.sum()\n",
    "        self.bigram_probs = bigram_counts / (bigram_counts.sum(axis=1, keepdims=True) + 1e-9)\n",
    "        self.trigram_probs = trigram_counts / (trigram_counts.sum(axis=2, keepdims=True) + 1e-9)\n",
    "        self.trained = True\n",
    "\n",
    "    def _get_pattern_probs(self, masked_word, guessed_letters):\n",
    "        length = len(masked_word)\n",
    "        pattern_probs = np.zeros(26)\n",
    "        num_matching_words = 0\n",
    "        if length in self.words_by_length:\n",
    "            pattern = masked_word.replace('_', '.')\n",
    "            try: pattern_regex = re.compile(f'^{pattern}$')\n",
    "            except re.error: return pattern_probs, 0\n",
    "            guessed_wrong = {l for l in guessed_letters if l not in masked_word}\n",
    "            matching_words = [w for w in self.words_by_length[length] if pattern_regex.match(w) and not any(c in guessed_wrong for c in w)]\n",
    "            num_matching_words = len(matching_words)\n",
    "            if matching_words:\n",
    "                letter_counts = Counter(c for w in matching_words for c in set(w) if c not in guessed_letters)\n",
    "                if letter_counts:\n",
    "                    total_counts = sum(letter_counts.values())\n",
    "                    for letter, count in letter_counts.items():\n",
    "                        pattern_probs[self.letter_to_idx[letter]] = count / total_counts\n",
    "        return pattern_probs, num_matching_words\n",
    "\n",
    "    def predict_letter_probabilities(self, masked_word, guessed_letters):\n",
    "        if not self.trained: return np.ones(26) / 26\n",
    "        \n",
    "        lambda1, lambda2, lambda3 = 0.1, 0.3, 0.6\n",
    "        presence_probs = np.zeros(26)\n",
    "        if masked_word.count('_') == 0: return np.zeros(26)\n",
    "\n",
    "        for i, char in enumerate(masked_word):\n",
    "            if char == '_':\n",
    "                prev_char = masked_word[i-1] if i > 0 and masked_word[i-1] != '_' else None\n",
    "                p_prev_char = masked_word[i-2] if i > 1 and masked_word[i-2] != '_' else None\n",
    "                prob_dist = self.unigram_probs\n",
    "                if prev_char:\n",
    "                    prev_idx = self.letter_to_idx[prev_char]\n",
    "                    bigram_dist = self.bigram_probs[prev_idx, :]\n",
    "                    prob_dist = (1 - lambda1) * bigram_dist + lambda1 * self.unigram_probs\n",
    "                if p_prev_char and prev_char:\n",
    "                    p_prev_idx = self.letter_to_idx[p_prev_char]\n",
    "                    prev_idx = self.letter_to_idx[prev_char]\n",
    "                    trigram_dist = self.trigram_probs[p_prev_idx, prev_idx, :]\n",
    "                    bigram_dist = self.bigram_probs[prev_idx, :]\n",
    "                    prob_dist = (lambda3 * trigram_dist) + (lambda2 * bigram_dist) + (lambda1 * self.unigram_probs)\n",
    "                presence_probs += prob_dist\n",
    "\n",
    "        if presence_probs.sum() > 0: presence_probs /= presence_probs.sum()\n",
    "        pattern_probs, num_matching_words = self._get_pattern_probs(masked_word, guessed_letters)\n",
    "\n",
    "        if pattern_probs.sum() > 0:\n",
    "            pattern_weight = max(0.5, min(0.95, 0.95 - (num_matching_words / 500.0)))\n",
    "            combined = pattern_weight * pattern_probs + (1.0 - pattern_weight) * presence_probs\n",
    "        else:\n",
    "            combined = presence_probs\n",
    "        \n",
    "        for letter in guessed_letters:\n",
    "            if letter in self.letter_to_idx:\n",
    "                combined[self.letter_to_idx[letter]] = 0\n",
    "        \n",
    "        if combined.sum() > 0:\n",
    "            combined /= combined.sum()\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def save(self, filename='improved_hmm_model.pkl'):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                ### CORRECTED KEYS TO SAVE ###\n",
    "                'unigram_probs': self.unigram_probs,\n",
    "                'bigram_probs': self.bigram_probs,\n",
    "                'trigram_probs': self.trigram_probs,\n",
    "                'words_by_length': self.words_by_length,\n",
    "                'alphabet': self.alphabet,\n",
    "                'letter_to_idx': self.letter_to_idx,\n",
    "                'smoothing': self.smoothing\n",
    "            }, f)\n",
    "        print(f\"Model saved to {filename}\")\n",
    "    \n",
    "    def load(self, filename='improved_hmm_model.pkl'):\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            ### CORRECTED KEYS TO LOAD ###\n",
    "            self.unigram_probs = data['unigram_probs']\n",
    "            self.bigram_probs = data['bigram_probs']\n",
    "            self.trigram_probs = data['trigram_probs']\n",
    "            self.words_by_length = data['words_by_length']\n",
    "            self.alphabet = data['alphabet']\n",
    "            self.letter_to_idx = data['letter_to_idx']\n",
    "            self.smoothing = data['smoothing']\n",
    "            self.trained = True\n",
    "        print(f\"Model loaded from {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c381035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPROVED ENVIRONMENT WITH BETTER REWARDS\n",
    "# ==============================================================================\n",
    "\n",
    "class ImprovedHangmanEnvironment:\n",
    "    \"\"\"Hangman environment with balanced reward function.\"\"\"\n",
    "    \n",
    "    def __init__(self, word_list, max_wrong=6):\n",
    "        self.word_list = word_list\n",
    "        self.max_wrong = max_wrong\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, word=None):\n",
    "        \"\"\"Reset for new game. Now handles dictionary or list of words.\"\"\"\n",
    "        if word is None:\n",
    "            ### MODIFICATION START ###\n",
    "            # Handle the case where self.word_list is a dictionary of words grouped by length.\n",
    "            # This fixes the crash during the initial self.reset() call in __init__.\n",
    "            if isinstance(self.word_list, dict):\n",
    "                # 1. Pick a random length (a key from the dictionary)\n",
    "                random_length = random.choice(list(self.word_list.keys()))\n",
    "                # 2. Pick a random word from that length's list\n",
    "                self.target_word = random.choice(self.word_list[random_length]).lower()\n",
    "            else:\n",
    "                # Original behavior for backward compatibility: assume a flat list of words\n",
    "                self.target_word = random.choice(self.word_list).lower()\n",
    "            ### MODIFICATION END ###\n",
    "        else:\n",
    "            self.target_word = word.lower()\n",
    "        \n",
    "        self.masked_word = ['_'] * len(self.target_word)\n",
    "        self.guessed_letters = set()\n",
    "        self.wrong_guesses = 0\n",
    "        self.repeated_guesses = 0\n",
    "        self.done = False\n",
    "        \n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Return current state.\"\"\"\n",
    "        return {\n",
    "            'masked_word': ''.join(self.masked_word),\n",
    "            'guessed_letters': self.guessed_letters.copy(),\n",
    "            'wrong_guesses': self.wrong_guesses,\n",
    "            'lives_left': self.max_wrong - self.wrong_guesses,\n",
    "            'done': self.done,\n",
    "            'target_word': self.target_word\n",
    "        }\n",
    "    \n",
    "    def step(self, letter):\n",
    "        \"\"\"Take action with BALANCED reward function.\"\"\"\n",
    "        letter = letter.lower()\n",
    "        \n",
    "        if letter in self.guessed_letters:\n",
    "            self.repeated_guesses += 1\n",
    "            reward = -0.2\n",
    "            info = {'repeated': True, 'correct': False}\n",
    "            return self.get_state(), reward, self.done, info\n",
    "        \n",
    "        self.guessed_letters.add(letter)\n",
    "        \n",
    "        if letter in self.target_word:\n",
    "            positions_revealed = 0\n",
    "            for i, char in enumerate(self.target_word):\n",
    "                if char == letter:\n",
    "                    self.masked_word[i] = letter\n",
    "                    positions_revealed += 1\n",
    "            \n",
    "            reward = 0.5 * positions_revealed\n",
    "            \n",
    "            if '_' not in self.masked_word:\n",
    "                self.done = True\n",
    "                reward += 10.0\n",
    "            \n",
    "            info = {'repeated': False, 'correct': True, 'positions': positions_revealed}\n",
    "        else:\n",
    "            self.wrong_guesses += 1\n",
    "            reward = -1.0\n",
    "            \n",
    "            if self.wrong_guesses >= self.max_wrong:\n",
    "                self.done = True\n",
    "                reward -= 2.0\n",
    "            \n",
    "            info = {'repeated': False, 'correct': False}\n",
    "        \n",
    "        return self.get_state(), reward, self.done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c3ee83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SIMPLIFIED NEURAL NETWORK\n",
    "# ==============================================================================\n",
    "\n",
    "class SimplifiedDQN(nn.Module):\n",
    "    \"\"\"Simpler, more efficient network architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(SimplifiedDQN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "34d5e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPROVED RL AGENT\n",
    "# ==============================================================================\n",
    "\n",
    "class ImprovedRLAgent:\n",
    "    \"\"\"RL agent with a dense, feature-engineered state representation.\"\"\"\n",
    "    \n",
    "    def __init__(self, hmm_model):\n",
    "        self.hmm = hmm_model\n",
    "        self.alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        self.letter_to_idx = {c: i for i, c in enumerate(self.alphabet)}\n",
    "        self.idx_to_letter = {i: c for i, c in enumerate(self.alphabet)}\n",
    "        \n",
    "        # State representation remains the same dense, 55-dimensional vector\n",
    "        self.state_size = 55\n",
    "        \n",
    "        ### MODIFICATION START ###\n",
    "        # 1. STRATEGIC ACTION SPACE\n",
    "        self.action_size = 5 # Actions correspond to picking the 1st, 2nd, ..., 5th best HMM guess\n",
    "        ### MODIFICATION END ###\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.qnetwork = SimplifiedDQN(self.state_size, self.action_size).to(self.device)\n",
    "        self.target_qnetwork = SimplifiedDQN(self.state_size, self.action_size).to(self.device)\n",
    "        self.target_qnetwork.load_state_dict(self.qnetwork.state_dict())\n",
    "        \n",
    "        # Hyperparameters are robust and remain the same from the previous fix\n",
    "        self.optimizer = optim.Adam(self.qnetwork.parameters(), lr=0.001)\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.batch_size = 128\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9995\n",
    "        \n",
    "        self.update_target_every = 10\n",
    "        self.losses = []\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "        self.episode_wins = []\n",
    "    \n",
    "    ### MODIFICATION START ###\n",
    "    def encode_state(self, state):\n",
    "        \"\"\"Encodes state using dense, meaningful features.\"\"\"\n",
    "        masked_word = state['masked_word']\n",
    "        guessed_letters = state['guessed_letters']\n",
    "        lives_left = state['lives_left']\n",
    "        word_len = len(masked_word)\n",
    "        \n",
    "        # Feature 1: Normalized lives left\n",
    "        lives_norm = np.array([lives_left / 6.0])\n",
    "        \n",
    "        # Feature 2: Normalized count of remaining blanks\n",
    "        blanks = masked_word.count('_')\n",
    "        blanks_norm = np.array([blanks / word_len if word_len > 0 else 0])\n",
    "        \n",
    "        # Feature 3: Normalized count of unique letters revealed\n",
    "        unique_revealed = len(set(c for c in masked_word if c != '_'))\n",
    "        unique_revealed_norm = np.array([unique_revealed / word_len if word_len > 0 else 0])\n",
    "        \n",
    "        # Feature 4: HMM probabilities (already a great dense feature)\n",
    "        hmm_probs = self.hmm.predict_letter_probabilities(masked_word, guessed_letters)\n",
    "        \n",
    "        # Feature 5: Guessed letters binary vector\n",
    "        guessed_vec = np.zeros(26)\n",
    "        for letter in guessed_letters:\n",
    "            guessed_vec[self.letter_to_idx[letter]] = 1.0\n",
    "        \n",
    "        # Concatenate all features into a dense vector\n",
    "        encoded = np.concatenate([\n",
    "            lives_norm,\n",
    "            blanks_norm,\n",
    "            unique_revealed_norm,\n",
    "            hmm_probs,\n",
    "            guessed_vec\n",
    "        ]).astype(np.float32)\n",
    "        \n",
    "        return encoded\n",
    "    \n",
    "    def get_ranked_letter_choices(self, state):\n",
    "        \"\"\"Uses HMM to get a ranked list of the best letters to guess.\"\"\"\n",
    "        hmm_probs = self.hmm.predict_letter_probabilities(state['masked_word'], state['guessed_letters'])\n",
    "        # Sort letter indices by probability in descending order\n",
    "        sorted_indices = np.argsort(hmm_probs)[::-1]\n",
    "        \n",
    "        # Filter out letters that have already been guessed\n",
    "        ranked_choices = [idx for idx in sorted_indices if self.idx_to_letter[idx] not in state['guessed_letters']]\n",
    "        \n",
    "        # Return the top N choices (or fewer if not enough are available)\n",
    "        return ranked_choices[:self.action_size]\n",
    "\n",
    "    # 3. UPDATED ACTION SELECTION\n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"Selects a strategic action (0-4) and translates it to a letter.\"\"\"\n",
    "        ranked_choices = self.get_ranked_letter_choices(state)\n",
    "        \n",
    "        # If there are no valid choices, default to a safe action\n",
    "        if not ranked_choices:\n",
    "            return 0, 0 # action_idx, letter_idx (e.g., 'a')\n",
    "\n",
    "        # Epsilon-greedy exploration\n",
    "        if training and random.random() < self.epsilon:\n",
    "            action_idx = random.randrange(len(ranked_choices))\n",
    "        else:\n",
    "            # Exploitation: use the Q-network\n",
    "            encoded_state = self.encode_state(state)\n",
    "            state_tensor = torch.FloatTensor(encoded_state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.qnetwork(state_tensor).cpu().numpy()[0]\n",
    "            \n",
    "            # Mask Q-values for actions that are not possible (e.g., if there are only 3 valid choices)\n",
    "            mask = np.full(self.action_size, -np.inf)\n",
    "            mask[:len(ranked_choices)] = 0\n",
    "            q_values += mask\n",
    "\n",
    "            action_idx = np.argmax(q_values)\n",
    "        \n",
    "        # Translate the strategic action index (e.g., 2) to the actual letter index (e.g., 14 for 'o')\n",
    "        letter_idx = ranked_choices[action_idx]\n",
    "        \n",
    "        return action_idx, letter_idx\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience.\"\"\"\n",
    "        encoded_state = self.encode_state(state)\n",
    "        encoded_next_state = self.encode_state(next_state)\n",
    "        self.memory.append((encoded_state, action, reward, encoded_next_state, done))\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Train on a batch of experiences using the Double DQN algorithm.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        actions = torch.LongTensor(np.array(actions)).to(self.device)\n",
    "        rewards = torch.FloatTensor(np.array(rewards)).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        dones = torch.FloatTensor(np.array(dones)).to(self.device)\n",
    "        \n",
    "        # Get Q-values for current states from the main network\n",
    "        current_q = self.qnetwork(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        ### MODIFICATION START: DOUBLE DQN LOGIC ###\n",
    "        with torch.no_grad():\n",
    "            # 1. Select the best action for the next_state using the MAIN network.\n",
    "            best_next_actions = self.qnetwork(next_states).argmax(1).unsqueeze(1)\n",
    "            \n",
    "            # 2. Evaluate that action's Q-value using the TARGET network.\n",
    "            # This decouples selection from evaluation.\n",
    "            target_q_values = self.target_qnetwork(next_states).gather(1, best_next_actions).squeeze()\n",
    "\n",
    "            # Calculate the final target Q-value for the Bellman equation\n",
    "            target_q = rewards + (1 - dones) * self.gamma * target_q_values\n",
    "        ### MODIFICATION END ###\n",
    "        \n",
    "        # Calculate loss and perform backpropagation\n",
    "        loss = F.mse_loss(current_q.squeeze(), target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.qnetwork.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"Decay epsilon.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network.\"\"\"\n",
    "        self.target_qnetwork.load_state_dict(self.qnetwork.state_dict())\n",
    "    \n",
    "    def save(self, filename='improved_rl_agent.pth'):\n",
    "        \"\"\"Save agent.\"\"\"\n",
    "        torch.save({\n",
    "            'qnetwork': self.qnetwork.state_dict(),\n",
    "            'target_qnetwork': self.target_qnetwork.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'stats': {\n",
    "                'rewards': self.episode_rewards,\n",
    "                'steps': self.episode_steps,\n",
    "                'losses': self.losses\n",
    "            }\n",
    "        }, filename)\n",
    "        print(f\"Agent saved to {filename}\")\n",
    "    \n",
    "    def load(self, filename='improved_rl_agent.pth'):\n",
    "        \"\"\"Load agent.\"\"\"\n",
    "        checkpoint = torch.load(filename, map_location=self.device)\n",
    "        self.qnetwork.load_state_dict(checkpoint['qnetwork'])\n",
    "        self.target_qnetwork.load_state_dict(checkpoint['target_qnetwork'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        if 'stats' in checkpoint:\n",
    "            self.episode_rewards = checkpoint['stats']['rewards']\n",
    "            self.episode_steps = checkpoint['stats']['steps']\n",
    "            self.losses = checkpoint['stats']['losses']\n",
    "        print(f\"Agent loaded from {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ff4a53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_improved_agent(agent, env, num_episodes=8000):\n",
    "    \"\"\"Train with the strategic action space and success rate logging.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING RL AGENT WITH STRATEGIC ACTION SPACE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Clear stats from previous runs, if any\n",
    "    agent.episode_rewards, agent.episode_steps, agent.episode_wins = [], [], []\n",
    "    \n",
    "    words_by_length = {l: ws for l, ws in env.word_list.items()}\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Curriculum learning setup... (This is unchanged)\n",
    "        if episode < 1500:   max_len = 6\n",
    "        elif episode < 3000: max_len = 8\n",
    "        elif episode < 5000: max_len = 11\n",
    "        else:                max_len = 25\n",
    "        \n",
    "        available_lengths = [l for l in words_by_length.keys() if l <= max_len]\n",
    "        if not available_lengths: available_lengths = list(words_by_length.keys())\n",
    "        \n",
    "        chosen_length = random.choice(available_lengths)\n",
    "        word = random.choice(words_by_length[chosen_length])\n",
    "        state = env.reset(word=word)\n",
    "\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not state['done']:\n",
    "            # This inner game loop is unchanged and correct from the last fix.\n",
    "            strategic_action, letter_action = agent.get_action(state, training=True)\n",
    "            letter_to_guess = agent.idx_to_letter[letter_action]\n",
    "            next_state, reward, done, info = env.step(letter_to_guess)\n",
    "            \n",
    "            is_wrong_guess = not info['correct'] and not info['repeated']\n",
    "            if is_wrong_guess:\n",
    "                hmm_probs = agent.hmm.predict_letter_probabilities(state['masked_word'], state['guessed_letters'])\n",
    "                prob_of_wrong_guess = hmm_probs[letter_action]\n",
    "                penalty = (1.0 - prob_of_wrong_guess) * 0.25 \n",
    "                reward -= penalty\n",
    "            \n",
    "            agent.remember(state, strategic_action, reward, next_state, done)\n",
    "            agent.replay()\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "        \n",
    "        ### MODIFICATION START ###\n",
    "        # After the episode ends, check if it was a win or loss.\n",
    "        is_win = '_' not in state['masked_word']\n",
    "        agent.episode_wins.append(1 if is_win else 0)\n",
    "        ### MODIFICATION END ###\n",
    "\n",
    "        # Standard updates\n",
    "        if episode % agent.update_target_every == 0:\n",
    "            agent.update_target_network()\n",
    "        agent.update_epsilon()\n",
    "        \n",
    "        agent.episode_rewards.append(total_reward)\n",
    "        agent.episode_steps.append(steps)\n",
    "        \n",
    "        # Updated logging block\n",
    "        if (episode + 1) % 200 == 0:\n",
    "            avg_reward = np.mean(agent.episode_rewards[-200:])\n",
    "            avg_steps = np.mean(agent.episode_steps[-200:])\n",
    "            \n",
    "            ### MODIFICATION START ###\n",
    "            # Calculate success rate over the last 200 episodes\n",
    "            success_rate = np.mean(agent.episode_wins[-200:])\n",
    "            \n",
    "            # Add success rate to the print statement (formatted as percentage)\n",
    "            print(f\"Episode {episode+1}/{num_episodes} | \"\n",
    "                  f\"Success Rate: {success_rate:.1%} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                  f\"Avg Steps: {avg_steps:.2f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f} | \"\n",
    "                  f\"Max Len: {max_len}\")\n",
    "            ### MODIFICATION END ###\n",
    "            \n",
    "    print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e78ea048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EVALUATION (Corrected)\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_improved_agent(agent, test_words, max_wrong=6):\n",
    "    \"\"\"Evaluate on test set.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EVALUATING IMPROVED AGENT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    wins = 0\n",
    "    total_wrong = 0\n",
    "    total_repeated = 0\n",
    "    \n",
    "    # Use a single environment instance for efficiency\n",
    "    env = ImprovedHangmanEnvironment([test_words[0]], max_wrong=max_wrong)\n",
    "    \n",
    "    for i, word in enumerate(test_words):\n",
    "        state = env.reset(word=word)\n",
    "        \n",
    "        while not state['done']:\n",
    "            ### MODIFICATION START ###\n",
    "            \n",
    "            # The agent's get_action method returns a tuple: (strategic_action, letter_action)\n",
    "            # We must unpack it into two separate variables.\n",
    "            strategic_action, letter_action = agent.get_action(state, training=False)\n",
    "            \n",
    "            # Now, use the correct variable (letter_action) to get the letter character.\n",
    "            letter = agent.idx_to_letter[letter_action]\n",
    "            \n",
    "            ### MODIFICATION END ###\n",
    "\n",
    "            state, reward, done, info = env.step(letter)\n",
    "        \n",
    "        if '_' not in state['masked_word']:\n",
    "            wins += 1\n",
    "        total_wrong += env.wrong_guesses\n",
    "        total_repeated += env.repeated_guesses\n",
    "        \n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"Evaluated {i+1}/{len(test_words)} words...\")\n",
    "    \n",
    "    num_games = len(test_words)\n",
    "    success_rate = wins / num_games\n",
    "    avg_wrong = total_wrong / num_games\n",
    "    avg_repeated = total_repeated / num_games\n",
    "    \n",
    "    # The scoring formula in the problem description is based on rates AND totals.\n",
    "    # It seems to be (Success Rate * 2000) not (Success Rate * num_games). Let's stick to the prompt.\n",
    "    # Also, the prompt uses a sample of 1000 games for the score, so we should scale.\n",
    "    scaling_factor = 1000 / num_games\n",
    "    final_score = (success_rate * 2000) - ((total_wrong * scaling_factor) * 5) - ((total_repeated * scaling_factor) * 2)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Games Played: {num_games}\")\n",
    "    print(f\"Success Rate: {success_rate:.2%}\")\n",
    "    print(f\"Average Wrong Guesses per Game: {avg_wrong:.2f}\")\n",
    "    print(f\"Average Repeated Guesses per Game: {avg_repeated:.2f}\")\n",
    "    print(f\"FINAL SCORE (scaled to 1000 games as per prompt): {final_score:.2f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return {\n",
    "        'success_rate': success_rate,\n",
    "        'avg_wrong': avg_wrong,\n",
    "        'avg_repeated': avg_repeated,\n",
    "        'final_score': final_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70689ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "IMPROVED HANGMAN SOLUTION\n",
      "======================================================================\n",
      "\n",
      "Loading improved HMM...\n",
      "Model loaded from improved_hmm_model.pkl\n",
      "\n",
      "Loading corpus...\n",
      "Loaded 50000 words\n",
      "Loaded and grouped 50000 words\n",
      "\n",
      "======================================================================\n",
      "TRAINING RL AGENT WITH STRATEGIC ACTION SPACE\n",
      "======================================================================\n",
      "Episode 200/8000 | Success Rate: 33.5% | Avg Reward: -3.01 | Avg Steps: 7.19 | Epsilon: 0.905 | Max Len: 6\n",
      "Episode 400/8000 | Success Rate: 35.5% | Avg Reward: -2.61 | Avg Steps: 7.04 | Epsilon: 0.819 | Max Len: 6\n",
      "Episode 600/8000 | Success Rate: 38.5% | Avg Reward: -2.21 | Avg Steps: 6.99 | Epsilon: 0.741 | Max Len: 6\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# MAIN\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"IMPROVED HANGMAN SOLUTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load HMM\n",
    "    print(\"\\nLoading improved HMM...\")\n",
    "    hmm = ImprovedHangmanHMM()\n",
    "    hmm.load('improved_hmm_model.pkl')\n",
    "    \n",
    "    # Load corpus\n",
    "    print(\"\\nLoading corpus...\")\n",
    "    with open('corpus.txt', 'r') as f:\n",
    "        corpus_words = [line.strip().lower() for line in f if line.strip()]\n",
    "    print(f\"Loaded {len(corpus_words)} words\")\n",
    "    \n",
    "    words_by_len_dict = {}\n",
    "    for word in corpus_words:\n",
    "        length = len(word)\n",
    "        if length not in words_by_len_dict:\n",
    "            words_by_len_dict[length] = []\n",
    "        words_by_len_dict[length].append(word)\n",
    "    print(f\"Loaded and grouped {len(corpus_words)} words\")\n",
    "    \n",
    "    # Pass the grouped dictionary to the environment\n",
    "    env = ImprovedHangmanEnvironment(words_by_len_dict, max_wrong=6)\n",
    "    agent = ImprovedRLAgent(hmm)\n",
    "    \n",
    "    train_improved_agent(agent, env, num_episodes=8000)\n",
    "    \n",
    "    agent.save('improved_rl_agent.pth')\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nLoading test set...\")\n",
    "    with open('test.txt', 'r') as f:\n",
    "        test_words = [line.strip().lower() for line in f if line.strip()]\n",
    "    \n",
    "    results = evaluate_improved_agent(agent, test_words)\n",
    "    \n",
    "    print(\"\\nâœ… COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AIML GPU)",
   "language": "python",
   "name": "aiml-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
